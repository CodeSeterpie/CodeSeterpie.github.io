---
title: "活動記録 2020/02/15"
last_modified_at: 2020-02-15T12:00:00+09:00
categories:
  - Blog
tags:
  - Kaggle
---

## コードのチェック機能と自動整形機能の追加
コードを簡単に見やすくするために、[PEP8(Python のコーディング規約)](https://pep8-ja.readthedocs.io/ja/latest/)に沿って、文法チェックをしてくれる[flake8](https://pypi.org/project/flake8/)と、自動整形してくれる[jupyterlab-code-formatter](https://pypi.org/project/jupyterlab-code-formatter/)を導入した。導入したpull requestは[こちら](/CodeSeterpie/CodeSeterpie/pull/33)。

使用方法は [/Kaggle/HousePrices/README.md](https://github.com/CodeSeterpie/CodeSeterpie/blob/develop/Kaggle/HousePrices/README.md) を参照。

## ニューラルネットワークモデル(Keras)を用いたデータ分析
Kerasライブラリを使って、ニューラルネットワークモデルを使ったデータ分析を実施した。結果としてはLightGBMを使用した勾配ブースティング木(GBDT)モデルよりも精度が悪く、処理速度も遅かった。  
とにかく動くように実装したのみであるため、特徴量の修正やハイパーパラメータの調整によって改善される見込みは十分にあると思う。

* 分析に使用したnotebook  
[/Kaggle/HousePrices/notebook/main/20200215/note_keras.ipynb](https://github.com/CodeSeterpie/CodeSeterpie/blob/develop/Kaggle/HousePrices/notebook/main/20200215/note_keras.ipynb)

## 実装方法を変更したXGBoostモデルとLightGBMモデルの比較
XGBoostモデルの実装方法を変更し、前回実装したLightGBMモデルのnotebookと処理結果を比較した。結果としては下の通りとなり、処理速度同等で精度は若干LightGBMが良かった。
|モデル|処理速度|バリデーションスコア|
|:---|---:|---:|
|XGBoost|1秒以下|0.1561(RMSE)|
|LightGBM|1秒以下|0.1410(RMSE)|

* 今回実装方法を変更したXGBoostのnotebook  
[/Kaggle/HousePrices/notebook/main/20200215/note_xgb.ipynb](https://github.com/CodeSeterpie/CodeSeterpie/blob/develop/Kaggle/HousePrices/notebook/main/20200215/note_xgb.ipynb)

* 前回実装したLightGBMのnotebook  
[/Kaggle/HousePrices/notebook/main/20200208/mainnote.ipynb](https://github.com/CodeSeterpie/CodeSeterpie/blob/develop/Kaggle/HousePrices/notebook/main/20200208/mainnote.ipynb)

## Kaggleへの挑戦
実際にプログラムを作成して[
House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview)に予測したデータを提出し、スコアを取得した。  

### 真の値と予測値との差異を可視化
モデルを使って予測した値と真の値がどの程度離れているかを可視化するために、グラフを作成した。

結果として、直感的に差異を見ることができるようになったが、どの特徴量が関係してくるのかは不明なままとなった。  
スコアを上げるためには、引き続きLime等のツールで結果の可視化を行っていく必要がある。

### 全ての特徴量をモデルに与えた場合の結果
前回は全てのカテゴリ変数と、HousePriceに関係するものを選別した数値変数を使って分析を行ったが、今回はすべてのカテゴリ変数と全ての数値変数をモデルに与えて学習を行ってみた。

結果はこれまでで最高のスコア(0.13573)をとなった。  
人が選別するよりも全てモデルに任せた方が良い結果となったのは、特徴量を作成する手間が省けて良いと思うが、これ以上スコアを上げるために何をしたらいいのか再度考える必要がある。

#### 提出した結果

<img src="/assets/images/posts/report_20200215/HousePriceScore_20200215.jpg" width="800">

#### ソースへのリンク
* 分析に使用したnotebook  
[/Kaggle/HousePrices/notebook/main/20200215/mainnote.ipynb](https://github.com/CodeSeterpie/CodeSeterpie/blob/develop/Kaggle/HousePrices/notebook/main/20200215/mainnote.ipynb)
* 提出したファイル  
[/Kaggle/HousePrices/output/main/20200215/submission.csv](https://github.com/CodeSeterpie/CodeSeterpie/blob/develop/Kaggle/HousePrices/output/main/20200215/submission.csv)

## 課題
今回の活動で出た課題は下の通り。
* [Titanicコンペの回析をできるようにしておきたい](https://github.com/CodeSeterpie/CodeSeterpie/issues/34)



