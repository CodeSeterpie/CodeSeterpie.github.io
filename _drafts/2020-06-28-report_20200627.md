---
title: "活動記録 2020/06/27 - HousePriceで目標としていたスコア0.12を達成(0.11992)"
last_modified_at: 2020-06-28T12:00:00+09:00
categories:
  - Blog
tags:
  - Kaggle
---

前回の活動でスコアを0.123台まで改善することができ、HousePriceで目標としている0.12まで後わずかとなった。今回はモデルのパラメータなどを微調整したり、Null値の扱いを変えたり、アンサンブルの方法を変えたりして、Kaggleへ結果を何度も提出し、トライ＆エラーを繰り返すことでスコアの改善を試みた。

結果としては、目標のスコア0.12を超える0.11992を得ることができ、目標を達成した。

具体的な修正内容は[今回の活動成果のプルリクエスト](https://github.com/CodeSeterpie/CodeSeterpie/pull/68/files#diff-950ad864b29d2230c1edf5f100b9dbd0)を参照。サマリーは下の通り。

* Null値を明確に0や"None"などの文字列で埋めた。
* カテゴリ変数の中でグレード(非常に良い、良い、普通、悪い)を表す変数の数値変換を極端なもの(5, 4, 3, 2, 1 → 16, 8, 4, 2, 1)に変更。
* モデルを作成するときの学習データの分割数を4→24に変更。
* XGBoostモデルの下のパラメータを変更
  * learning_rateに0.12を設定
  * objectiveにreg:linearを設定
  * trainの引数num_boost_roundを100→170に変更
* LightGBMとXGBoostのアンサンブルの方法を変更
  * 【変更前】バリデーションデータでのスコアの逆数を重さとして、平均値を取得
  * 【変更後】LightGBMとXGBoostそれぞれのバリデーションデータでのスコアを比較し、良い方だけを採用して平均値を取得。どちらのスコアも0.15より悪ければどちらも採用しない。

#### 提出した結果

<img src="/assets/images/posts/report_20200627/HousePriceScore_20200627.jpg" width="800">

---

### メモ
* パラメータの調整を手動で行ったが、本来は[optuna](https://preferred.jp/ja/projects/optuna/)などのパラメータ自動最適化ライブラリを使うべき。今回も使ってみようとしたが、これまで作成してきたコードを大きく変える必要が合ったため断念した。新しいコンペに挑戦する際は最終的なパラメータ調整を想定したコードを作成していきたい。
* 参考Website [LightGBM初心者がOptunaとKFoldを組み合わてみた](https://www.hirayuki.com/kaggle-zakki/lightgbm-optuna-kfold-introduction)
* 参考Website [optunaのGighub](https://github.com/optuna/optuna)

